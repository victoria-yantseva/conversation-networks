{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d448bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "import numpy as np\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "from matplotlib.pyplot import figure\n",
    "import math\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import shapiro \n",
    "from scipy.stats import lognorm\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "import pymannkendall\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c74b328",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load the data\n",
    "##############################################\n",
    "\n",
    "\n",
    "fb=pd.read_json(\"fb.json\")\n",
    "\n",
    "#upload the network and edgelist data depending on the time interval chosen (14 / 27 / 54 min)\n",
    "g = pd.read_csv(\"conversations_graph_27min.csv\", names = [\"convid\", \"n_nodes\", \"n_edges\", \"ideg\", \"odeg\", \"deg\", \"avg_clust\", \"recip\", \"dens\"])\n",
    "g.convid = g.convid.astype(\"str\")\n",
    "\n",
    "e = pd.read_csv (\"conversations_edges_27min.csv\", names = [\"convid\", \"sender\", \"recepient\", \"timestamp\"])\n",
    "e\n",
    "\n",
    "el = pd.read_csv (\"conversation_edgelist27min.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f518df9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fix conversation ids and dates\n",
    "################################################\n",
    "\n",
    "fb['delta'] = (fb['date']-fb['date'].shift()).fillna(pd.Timedelta('0 days'))\n",
    "fb['delta'] = fb['delta'].dt.total_seconds() / 60\n",
    "fb.delta = fb.delta.astype(int)\n",
    "\n",
    "fb[\"convid\"] = fb[\"convid\"].astype(str)\n",
    "fb.convid = fb.convid.apply(lambda x: re.sub(\"\\\\.0\", \"\", x))\n",
    "convids = fb.convid.unique().tolist()\n",
    "convids.remove(\"nan\")\n",
    "\n",
    "fb = fb.assign (date24h = fb[\"date\"].dt.floor(\"24h\")).reset_index (drop = True)\n",
    "\n",
    "e.timestamp = pd.to_datetime(e.timestamp, format=\"%Y-%m-%d %H:%M\")\n",
    "e.timestamp\n",
    "\n",
    "el.time_sender = pd.to_datetime(el.time_sender, format=\"%Y-%m-%d %H:%M\")\n",
    "el.time_recepient = pd.to_datetime(el.time_recepient, format=\"%Y-%m-%d %H:%M\")\n",
    "el  = el.merge(e[[\"convid\", \"sender\", \"recepient\"]], left_on = [\"sender\", \"recepient\"], right_on = [\"sender\", \"recepient\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb408d53",
   "metadata": {},
   "source": [
    "# SUMMARY STATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "284ced1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      num_am  num_mm  num_all  vader_score  num_posts  am_density  mm_density  \\\n",
      "0          2      10       25    -0.184269        146    0.013699    0.068493   \n",
      "1          0       0        1     0.107300          3    0.000000    0.000000   \n",
      "2          0       0        3    -0.557833          3    0.000000    0.000000   \n",
      "3          0       0        0    -0.299150          2    0.000000    0.000000   \n",
      "4          0       1        1     0.111300          4    0.000000    0.250000   \n",
      "...      ...     ...      ...          ...        ...         ...         ...   \n",
      "9299       0       0        1    -0.957050          2    0.000000    0.000000   \n",
      "9300       0       2        2     0.425950          2    0.000000    1.000000   \n",
      "9301       0       0        0    -0.577550          2    0.000000    0.000000   \n",
      "9302       0       0        6     0.338025          4    0.000000    0.000000   \n",
      "9303       0       1        1     0.013333          3    0.000000    0.333333   \n",
      "\n",
      "      all_density  am_to_all  mm_to_all  ...   diff_hours  time_diff  n_nodes  \\\n",
      "0        0.171233       0.08        0.4  ...   132.216667       14.0      146   \n",
      "1        0.333333       0.00        0.0  ...     0.100000        3.0        3   \n",
      "2        1.000000       0.00        0.0  ...    14.733333      499.0        3   \n",
      "3        0.000000        NaN        NaN  ...     1.400000       84.0        2   \n",
      "4        0.250000       0.00        1.0  ...     1.866667       27.0        4   \n",
      "...           ...        ...        ...  ...          ...        ...      ...   \n",
      "9299     0.500000       0.00        0.0  ...     0.366667       22.0        2   \n",
      "9300     1.000000       0.00        1.0  ...     0.183333       11.0        2   \n",
      "9301     0.000000        NaN        NaN  ...     0.033333        2.0        2   \n",
      "9302     1.500000       0.00        0.0  ...    69.516667     1108.0        4   \n",
      "9303     0.333333       0.00        1.0  ...  1288.900000    38667.0        3   \n",
      "\n",
      "      n_edges      ideg      odeg       deg avg_clust  recip      dens  \n",
      "0         888  0.041946  0.041946  0.083892  0.278248    0.0  0.041946  \n",
      "1           3  0.500000  0.500000  1.000000  0.500000    0.0  0.500000  \n",
      "2           2  0.333333  0.333333  0.666667  0.000000    0.0  0.333333  \n",
      "3           1  0.500000  0.500000  1.000000  0.000000    0.0  0.500000  \n",
      "4           3  0.250000  0.250000  0.500000  0.000000    0.0  0.250000  \n",
      "...       ...       ...       ...       ...       ...    ...       ...  \n",
      "9299        1  0.500000  0.500000  1.000000  0.000000    0.0  0.500000  \n",
      "9300        1  0.500000  0.500000  1.000000  0.000000    0.0  0.500000  \n",
      "9301        1  0.500000  0.500000  1.000000  0.000000    0.0  0.500000  \n",
      "9302        3  0.250000  0.250000  0.500000  0.000000    0.0  0.250000  \n",
      "9303        2  0.333333  0.333333  0.666667  0.000000    0.0  0.333333  \n",
      "\n",
      "[9304 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "#Create CONVERSATION statistics (summary df)\n",
    "###############################################\n",
    "fb_conv = fb[fb.convid != \"nan\"].groupby(\"convid\").agg({\"num_am\": 'sum', \"num_mm\": 'sum', \"num_all\": 'sum', \n",
    "                                                            \"vader_score\": 'mean', 'stance_label':\n",
    "                                                            'count'}).rename (columns ={\"stance_label\": \"num_posts\"})\n",
    "\n",
    "#links\n",
    "fb_conv['am_density'] = fb_conv.num_am.div(fb_conv [\"num_posts\"])\n",
    "fb_conv['mm_density'] = fb_conv.num_mm.div(fb_conv[\"num_posts\"])\n",
    "fb_conv['all_density'] = fb_conv.num_all.div(fb_conv [\"num_posts\"])\n",
    "\n",
    "fb_conv['am_to_all'] = fb_conv.num_am.div(fb_conv [\"num_all\"])\n",
    "fb_conv['mm_to_all'] = fb_conv.num_mm.div(fb_conv [\"num_all\"])\n",
    "\n",
    "#stances\n",
    "\n",
    "threads_neg = fb[(fb.convid != \"nan\")&(fb.stance_label == \"neg\")].groupby (\"convid\").agg({'stance_label' : 'count',\n",
    "                                                                                              'stance_prob' : 'mean'})\n",
    "threads_neg = threads_neg.rename (columns={\"stance_label\": \"stance_label_neg\", \"stance_prob\": \"mean_stance_prob_neg\"})\n",
    "\n",
    "\n",
    "#POSITIVE STANCES\n",
    "\n",
    "threads_nonneg = fb[(fb.convid != \"nan\")&(fb.stance_label == \"nonneg\")].groupby (\"convid\").agg({'stance_label' : \n",
    "                                                                                                    'count', \n",
    "                                                                                                    'stance_prob' : \n",
    "                                                                                                    'mean'})\n",
    "threads_nonneg = threads_nonneg.rename (columns={\"stance_label\": \"stance_label_nonneg\", \"stance_prob\": \n",
    "                                                 \"mean_stance_prob_nonneg\"})\n",
    "\n",
    "#\n",
    "fb_conv = pd.concat([fb_conv, threads_neg, threads_nonneg], axis = 1)\n",
    "\n",
    "fb_conv['neg_label_ratio'] = fb_conv.stance_label_neg.div(fb_conv [\"num_posts\"])\n",
    "fb_conv['nonneg_label_ratio'] = fb_conv.stance_label_nonneg.div(fb_conv [\"num_posts\"])\n",
    "\n",
    "fb_conv = fb_conv.assign (convid = fb_conv.index).reset_index (drop = True)\n",
    "\n",
    "\n",
    "#conversation durations\n",
    "diffdf = (fb[fb.convid != \"nan\"].groupby(['convid'])['date']\n",
    "         .agg(lambda x: x.iat[-1] - x.iat[0])\n",
    "         .reset_index(name='diff'))\n",
    "\n",
    "el_sumdiff = el[[\"convid\", \"time_diff\"]].groupby (\"convid\", as_index=False).median(\"time_diff\")\n",
    "el_sumdiff.convid = el_sumdiff.convid.astype(str)\n",
    "el_sumdiff.convid = el_sumdiff.convid.apply(lambda x: re.sub(\"\\\\.0\", \"\", x))\n",
    "\n",
    "\n",
    "#fb_conv = pd.concat([fb_conv, diffdf], axis = 1)\n",
    "fb_conv = fb_conv.merge(diffdf, right_on = \"convid\", left_on = \"convid\")\n",
    "\n",
    "\n",
    "fb_conv[\"diff_hours\"] = fb_conv[\"diff\"]/ np.timedelta64(1, 'h')\n",
    "\n",
    "fb_conv = fb_conv.merge(el_sumdiff, left_on = \"convid\", right_on = \"convid\")\n",
    "\n",
    "#add network metrics\n",
    "\n",
    "fb_conv = fb_conv.merge(g, left_on = \"convid\", right_on = \"convid\")\n",
    "\n",
    "print(fb_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d50ee9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.48194325021496\n",
      "5.0\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(fb_conv.num_posts))\n",
    "print(np.median(fb_conv.num_posts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3f110be",
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_conv.to_excel(\"fb_conv_27.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3ccb72",
   "metadata": {},
   "source": [
    "# Mann-Whitney U test - LINKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "730add83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161\n",
      "975\n",
      "MannwhitneyuResult(statistic=75755.5, pvalue=0.7606930478830192)\n",
      "MannwhitneyuResult(statistic=79206.0, pvalue=0.4261540148875254)\n",
      "MannwhitneyuResult(statistic=63238.0, pvalue=0.5506410874185431)\n",
      "MannwhitneyuResult(statistic=30710.0, pvalue=0.024500498283528727)\n",
      "MannwhitneyuResult(statistic=75199.0, pvalue=0.00020339711072298082)\n",
      "MannwhitneyuResult(statistic=31310.0, pvalue=0.043901323980415824)\n",
      "MannwhitneyuResult(statistic=74782.5, pvalue=0.15541855143956934)\n",
      "MannwhitneyuResult(statistic=68875.5, pvalue=0.00582999734584923)\n",
      "MannwhitneyuResult(statistic=69120.5, pvalue=0.007070565817020889)\n",
      "MannwhitneyuResult(statistic=86744.5, pvalue=0.014242819167947167)\n"
     ]
    }
   ],
   "source": [
    "# AM VERSUS MM CONVERSATIONS\n",
    "######################################################\n",
    "\n",
    "am_conv = fb_conv[(fb_conv.am_density>0.2) & (fb_conv.mm_density == 0)] #other values tested: 0.2, 0.4, 0.6\n",
    "print(len(am_conv))\n",
    "mm_conv = fb_conv[(fb_conv.mm_density>0.2) & (fb_conv.am_density == 0)] #other values tested: 0.2, 0.4, 0.6\n",
    "print(len(mm_conv))\n",
    "\n",
    "#conversations with am links are longer?\n",
    "mw_hours = mannwhitneyu(x=am_conv.diff_hours, y=mm_conv.diff_hours, alternative='greater') #‘greater’: the distribution underlying x is stochastically greater than the distribution underlying y, i.e. F(u) < G(u) for all u.\n",
    "print(mw_hours)\n",
    "\n",
    "#conversations with am links have higher sentiments? +++\n",
    "mw_vader = mannwhitneyu(x=am_conv.vader_score, y=mm_conv.vader_score, alternative='greater') \n",
    "print(mw_vader)\n",
    "\n",
    "#conversations with am links have higher negative stance probability? \n",
    "mw_negprob = mannwhitneyu(x=am_conv.mean_stance_prob_neg.dropna(), y=mm_conv.mean_stance_prob_neg.dropna(), alternative='greater') \n",
    "print(mw_negprob)\n",
    "\n",
    "#conversations with am links have lower non-negative stance probability? \n",
    "mw_nonnegprob = mannwhitneyu(x=am_conv.mean_stance_prob_nonneg.dropna(), y=mm_conv.mean_stance_prob_nonneg.dropna(), alternative='less') \n",
    "print(mw_nonnegprob)\n",
    "\n",
    "#conversations with am links have higher negative stance ratio? ++++\n",
    "mw_negprob = mannwhitneyu(x=am_conv.neg_label_ratio.dropna(), y=mm_conv.neg_label_ratio.dropna(), alternative='greater') \n",
    "print(mw_negprob)\n",
    "\n",
    "#conversations with am links have lower non-negative stance ratio?\n",
    "mw_nonnegprob = mannwhitneyu(x=am_conv.nonneg_label_ratio.dropna(), y=mm_conv.nonneg_label_ratio.dropna(), alternative='less') \n",
    "print(mw_nonnegprob)\n",
    "\n",
    "#conversations with am links have lower clustering?\n",
    "mw_clust = mannwhitneyu(x=am_conv.avg_clust, y=mm_conv.avg_clust, alternative='less') \n",
    "print(mw_clust)\n",
    "\n",
    "#conversations with am links have lower num nodes?\n",
    "mw_nodes = mannwhitneyu(x=am_conv.n_nodes, y=mm_conv.n_nodes, alternative='less') \n",
    "print(mw_nodes)\n",
    "\n",
    "#conversations with am links have lower num edges?\n",
    "mw_edges = mannwhitneyu(x=am_conv.n_edges, y=mm_conv.n_edges, alternative='less') \n",
    "print(mw_edges)\n",
    "\n",
    "#conversations with am links have higher density?\n",
    "mw_dens = mannwhitneyu(x=am_conv.dens, y=mm_conv.dens, alternative='greater') \n",
    "print(mw_dens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac61e4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "849\n",
      "4602\n",
      "MannwhitneyuResult(statistic=2357484.0, pvalue=4.505820100049835e-22)\n",
      "MannwhitneyuResult(statistic=1933359.0, pvalue=0.3158998439276627)\n",
      "MannwhitneyuResult(statistic=1393807.0, pvalue=9.780850870734706e-05)\n",
      "MannwhitneyuResult(statistic=602940.0, pvalue=0.002980911955877018)\n",
      "MannwhitneyuResult(statistic=1240670.0, pvalue=1.0025508822275686e-16)\n",
      "MannwhitneyuResult(statistic=656181.0, pvalue=2.1578556170069625e-10)\n",
      "MannwhitneyuResult(statistic=1974236.5, pvalue=0.28221874418687287)\n",
      "MannwhitneyuResult(statistic=2101005.0, pvalue=0.00012166453842038357)\n",
      "MannwhitneyuResult(statistic=2086292.0, pvalue=0.0004908582587693855)\n",
      "MannwhitneyuResult(statistic=1777798.5, pvalue=3.2681650095619783e-06)\n"
     ]
    }
   ],
   "source": [
    "# CONVERSATIONS WITHOUT AND WITH ANY TYPE OF LINKS\n",
    "#######################################################\n",
    "\n",
    "all_conv = fb_conv[fb_conv.all_density>0.6] #other values tested: 0.2, 0.4, 0.6\n",
    "\n",
    "none_conv = fb_conv[fb_conv.all_density==0]\n",
    "#none_conv = none_conv[0:300]    \n",
    "\n",
    "print(len(all_conv))\n",
    "print(len(none_conv))\n",
    "\n",
    "#conversations with any links are longer? ++++++\n",
    "mw_hours = mannwhitneyu(x=all_conv.diff_hours, y=none_conv.diff_hours, alternative='greater') #‘greater’: the distribution underlying x is stochastically greater than the distribution underlying y, i.e. F(u) < G(u) for all u.\n",
    "print(mw_hours)\n",
    "\n",
    "#conversations with any links have lower sentiments? \n",
    "mw_vader = mannwhitneyu(x=all_conv.vader_score, y=none_conv.vader_score, alternative='less') \n",
    "print(mw_vader)\n",
    "\n",
    "#conversations with any links have higher negative stance probability?  +++++\n",
    "mw_negprob = mannwhitneyu(x=all_conv.mean_stance_prob_neg.dropna(), y=none_conv.mean_stance_prob_neg.dropna(), alternative='less') \n",
    "print(mw_negprob)\n",
    "\n",
    "#conversations with any links have lower non-negative stance probability? +++++ \n",
    "mw_nonnegprob = mannwhitneyu(x=all_conv.mean_stance_prob_nonneg.dropna(), y=none_conv.mean_stance_prob_nonneg.dropna(), alternative='greater') \n",
    "print(mw_nonnegprob)\n",
    "\n",
    "#conversations with any links have lower negative negative stance ratio? +++++\n",
    "mw_negprob = mannwhitneyu(x=all_conv.neg_label_ratio.dropna(), y=none_conv.neg_label_ratio.dropna(), alternative='less') \n",
    "print(mw_negprob)\n",
    "\n",
    "#conversations with any links have higher non-negative stance ratio? +++++\n",
    "mw_nonnegprob = mannwhitneyu(x=all_conv.nonneg_label_ratio.dropna(), y=none_conv.nonneg_label_ratio.dropna(), alternative='greater') \n",
    "print(mw_nonnegprob)\n",
    "\n",
    "#conversations with any links have higher clustering? \n",
    "mw_clust = mannwhitneyu(x=all_conv.avg_clust, y=none_conv.avg_clust, alternative='greater') \n",
    "print(mw_clust)\n",
    "\n",
    "#conversations with any links have lhigher num nodes? +++++++\n",
    "mw_nodes = mannwhitneyu(x=all_conv.n_nodes, y=none_conv.n_nodes, alternative='greater') \n",
    "print(mw_nodes)\n",
    "\n",
    "#conversations with any links have higher num edges? +++++++++\n",
    "mw_edges = mannwhitneyu(x=all_conv.n_edges, y=none_conv.n_edges, alternative='greater') \n",
    "print(mw_edges) \n",
    "\n",
    "#conversations with am links have higher density? ++++++\n",
    "mw_dens = mannwhitneyu(x=all_conv.dens, y=none_conv.dens, alternative='less') \n",
    "print(mw_dens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f355bc6",
   "metadata": {},
   "source": [
    "# ALT RIGHT LINKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffb9cae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5054271896829662\n",
      "0.09274583557227298\n",
      "0.28694250403009136\n",
      "14366\n",
      "1147\n",
      "0.07984129193930113\n",
      "14366\n"
     ]
    }
   ],
   "source": [
    "#how many conversations have any link?\n",
    "print(len(fb.convid[fb.num_all>0].unique())/len(fb.convid.unique()))\n",
    "\n",
    "#how many conversations have an am link?\n",
    "print(len(fb.convid[fb.num_am>0].unique())/len(fb.convid.unique()))\n",
    "\n",
    "#how many conversations have a mm link?\n",
    "print(len(fb.convid[fb.num_mm>0].unique())/len(fb.convid.unique()))\n",
    "\n",
    "\n",
    "#how many unique users share am content?\n",
    "\n",
    "print(len(fb.userid.unique()))\n",
    "print(len(fb.userid[fb.num_am>0].unique()))\n",
    "print(len(fb.userid[fb.num_am>0].unique())/len(fb.userid.unique()))\n",
    "print(len(fb.userid.unique()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
